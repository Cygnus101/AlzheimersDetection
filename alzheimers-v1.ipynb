{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5962731,"sourceType":"datasetVersion","datasetId":3419493},{"sourceId":11478294,"sourceType":"datasetVersion","datasetId":7194061}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Libraries\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport numpy as np\n\n# Define Paths\ndataset_dir = \"/kaggle/input/imagesoasis/Data\"  # Root folder containing class folders\n# Adjust class names to match actual folder names\nclasses = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n\n# Load Data with Correct Folder Names\nimage_paths, labels = [], []\nfor class_label, class_name in enumerate(classes):\n    class_dir = os.path.join(dataset_dir, class_name)\n    if not os.path.exists(class_dir):\n        print(f\"Error: Folder {class_dir} does not exist.\")\n        continue\n    files = glob.glob(f\"{class_dir}/*.jpg\")  # Adjust extension if needed\n    print(f\"Class: {class_name}, Files Found: {len(files)}\")  # Debug: Count files\n    for file_path in files:\n        image_paths.append(file_path)\n        labels.append(class_label)\n\n# Proceed with the pipeline if files are found\nif len(image_paths) == 0:\n    raise ValueError(\"No images found. Check dataset folder names or file paths.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T00:57:51.957438Z","iopub.execute_input":"2025-04-22T00:57:51.958121Z","iopub.status.idle":"2025-04-22T00:57:53.949369Z","shell.execute_reply.started":"2025-04-22T00:57:51.958079Z","shell.execute_reply":"2025-04-22T00:57:53.948532Z"}},"outputs":[{"name":"stdout","text":"Class: Mild Dementia, Files Found: 5002\nClass: Moderate Dementia, Files Found: 488\nClass: Non Demented, Files Found: 67222\nClass: Very mild Dementia, Files Found: 13725\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Split Dataset\ntrain_paths, test_paths, train_labels, test_labels = train_test_split(\n    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n)\n\nprint(f\"Total images: {len(image_paths)}\")\nprint(f\"Training images: {len(train_paths)}, Testing images: {len(test_paths)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T00:57:53.950166Z","iopub.execute_input":"2025-04-22T00:57:53.950519Z","iopub.status.idle":"2025-04-22T00:57:54.029778Z","shell.execute_reply.started":"2025-04-22T00:57:53.950499Z","shell.execute_reply":"2025-04-22T00:57:54.028996Z"}},"outputs":[{"name":"stdout","text":"Total images: 86437\nTraining images: 69149, Testing images: 17288\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.preprocessing import LabelBinarizer\n\n# Normalize image paths and labels\ntrain_paths = np.array(train_paths)\ntest_paths = np.array(test_paths)\n\n# One-hot encode the labels\nlabel_binarizer = LabelBinarizer()\ntrain_labels = label_binarizer.fit_transform(train_labels)\ntest_labels = label_binarizer.transform(test_labels)\n\n# Preprocessing Function for Images\nIMG_HEIGHT, IMG_WIDTH = 224, 224\n\ndef preprocess_image(image_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n    image = image / 255.0  # Normalize to [0, 1]\n    return image\n\n# Load Dataset\ndef load_dataset(image_paths, labels):\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    dataset = dataset.map(lambda x, y: (preprocess_image(x), y))\n    return dataset\n\n# Prepare Train and Test Datasets\ntrain_dataset = load_dataset(train_paths, train_labels)\ntest_dataset = load_dataset(test_paths, test_labels)\n\n# Batch and Shuffle the Datasets\nBATCH_SIZE = 32\ntrain_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T00:57:54.031392Z","iopub.execute_input":"2025-04-22T00:57:54.031621Z","iopub.status.idle":"2025-04-22T00:58:09.227972Z","shell.execute_reply.started":"2025-04-22T00:57:54.031604Z","shell.execute_reply":"2025-04-22T00:58:09.227183Z"}},"outputs":[{"name":"stderr","text":"2025-04-22 00:57:55.811859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745283476.013474      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745283476.069884      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1745283489.033454      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1745283489.034242      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras import layers, models, callbacks\nimport os\n\nbase_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg')\nfor layer in base_model.layers:\n    layer.trainable = False\nfor layer in base_model.layers[-2:]:\n    layer.trainable = True\n\n# Build model\nmodel = models.Sequential([\n    base_model,\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(4, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Callbacks\ncheckpoint_path = \"best_model.keras\"\n\ncallbacks_list = [\n    callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n    callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n]\n\n# Train model\nmodel.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=30,\n    callbacks=callbacks_list\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T02:25:02.306921Z","iopub.execute_input":"2025-04-22T02:25:02.307493Z","execution_failed":"2025-04-22T03:19:08.375Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7736 - loss: 0.7195\nEpoch 1: val_loss improved from inf to 0.66109, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 109ms/step - accuracy: 0.7736 - loss: 0.7195 - val_accuracy: 0.7777 - val_loss: 0.6611\nEpoch 2/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7788 - loss: 0.6626\nEpoch 2: val_loss improved from 0.66109 to 0.63427, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7788 - loss: 0.6626 - val_accuracy: 0.7777 - val_loss: 0.6343\nEpoch 3/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7788 - loss: 0.6481\nEpoch 3: val_loss improved from 0.63427 to 0.62775, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7788 - loss: 0.6481 - val_accuracy: 0.7777 - val_loss: 0.6278\nEpoch 4/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7792 - loss: 0.6447\nEpoch 4: val_loss improved from 0.62775 to 0.62727, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7792 - loss: 0.6447 - val_accuracy: 0.7777 - val_loss: 0.6273\nEpoch 5/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7791 - loss: 0.6381\nEpoch 5: val_loss did not improve from 0.62727\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 104ms/step - accuracy: 0.7791 - loss: 0.6381 - val_accuracy: 0.7777 - val_loss: 0.6458\nEpoch 6/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7792 - loss: 0.6374\nEpoch 6: val_loss improved from 0.62727 to 0.62129, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7792 - loss: 0.6374 - val_accuracy: 0.7777 - val_loss: 0.6213\nEpoch 7/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7794 - loss: 0.6359\nEpoch 7: val_loss improved from 0.62129 to 0.61434, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7794 - loss: 0.6359 - val_accuracy: 0.7777 - val_loss: 0.6143\nEpoch 8/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7788 - loss: 0.6343\nEpoch 8: val_loss did not improve from 0.61434\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 104ms/step - accuracy: 0.7788 - loss: 0.6343 - val_accuracy: 0.7777 - val_loss: 0.6162\nEpoch 9/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7793 - loss: 0.6317\nEpoch 9: val_loss improved from 0.61434 to 0.61124, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 105ms/step - accuracy: 0.7793 - loss: 0.6317 - val_accuracy: 0.7777 - val_loss: 0.6112\nEpoch 10/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7791 - loss: 0.6329\nEpoch 10: val_loss improved from 0.61124 to 0.60761, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 105ms/step - accuracy: 0.7791 - loss: 0.6329 - val_accuracy: 0.7777 - val_loss: 0.6076\nEpoch 11/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7784 - loss: 0.6304\nEpoch 11: val_loss improved from 0.60761 to 0.60586, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 105ms/step - accuracy: 0.7784 - loss: 0.6304 - val_accuracy: 0.7777 - val_loss: 0.6059\nEpoch 12/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7791 - loss: 0.6273\nEpoch 12: val_loss did not improve from 0.60586\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7791 - loss: 0.6273 - val_accuracy: 0.7777 - val_loss: 0.6059\nEpoch 13/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7792 - loss: 0.6318\nEpoch 13: val_loss improved from 0.60586 to 0.60517, saving model to best_model.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7792 - loss: 0.6318 - val_accuracy: 0.7777 - val_loss: 0.6052\nEpoch 14/30\n\u001b[1m1166/2161\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 84ms/step - accuracy: 0.7796 - loss: 0.6284","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model2 = models.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(128, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(len(classes), activation='softmax')  \n    ])\n\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Callbacks\ncheckpoint_path = \"best_model2.keras\"\n\ncallbacks_list = [\n    callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n    callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n]\n\n# Train model\nmodel.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=30,\n    callbacks=callbacks_list\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T01:39:44.603874Z","iopub.execute_input":"2025-04-22T01:39:44.604231Z","iopub.status.idle":"2025-04-22T02:25:02.305492Z","shell.execute_reply.started":"2025-04-22T01:39:44.604208Z","shell.execute_reply":"2025-04-22T02:25:02.304591Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7787 - loss: 0.6342\nEpoch 1: val_loss improved from inf to 0.61043, saving model to best_model2.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7787 - loss: 0.6342 - val_accuracy: 0.7777 - val_loss: 0.6104\nEpoch 2/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7792 - loss: 0.6334\nEpoch 2: val_loss did not improve from 0.61043\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 104ms/step - accuracy: 0.7792 - loss: 0.6334 - val_accuracy: 0.7777 - val_loss: 0.6200\nEpoch 3/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7791 - loss: 0.6345\nEpoch 3: val_loss improved from 0.61043 to 0.60496, saving model to best_model2.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7791 - loss: 0.6345 - val_accuracy: 0.7777 - val_loss: 0.6050\nEpoch 4/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7783 - loss: 0.6373\nEpoch 4: val_loss did not improve from 0.60496\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7783 - loss: 0.6374 - val_accuracy: 0.7777 - val_loss: 0.6071\nEpoch 5/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7792 - loss: 0.6328\nEpoch 5: val_loss did not improve from 0.60496\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 104ms/step - accuracy: 0.7792 - loss: 0.6328 - val_accuracy: 0.7777 - val_loss: 0.6105\nEpoch 6/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7795 - loss: 0.6347\nEpoch 6: val_loss did not improve from 0.60496\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7795 - loss: 0.6347 - val_accuracy: 0.7777 - val_loss: 0.6055\nEpoch 7/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7788 - loss: 0.6348\nEpoch 7: val_loss improved from 0.60496 to 0.60035, saving model to best_model2.keras\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 105ms/step - accuracy: 0.7788 - loss: 0.6348 - val_accuracy: 0.7777 - val_loss: 0.6004\nEpoch 8/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7788 - loss: 0.6387\nEpoch 8: val_loss did not improve from 0.60035\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 104ms/step - accuracy: 0.7788 - loss: 0.6387 - val_accuracy: 0.7777 - val_loss: 0.6300\nEpoch 9/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7788 - loss: 0.6494\nEpoch 9: val_loss did not improve from 0.60035\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 104ms/step - accuracy: 0.7788 - loss: 0.6494 - val_accuracy: 0.7777 - val_loss: 0.6347\nEpoch 10/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7788 - loss: 0.6475\nEpoch 10: val_loss did not improve from 0.60035\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 104ms/step - accuracy: 0.7788 - loss: 0.6476 - val_accuracy: 0.7777 - val_loss: 0.6417\nEpoch 11/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7789 - loss: 0.6472\nEpoch 11: val_loss did not improve from 0.60035\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 104ms/step - accuracy: 0.7789 - loss: 0.6472 - val_accuracy: 0.7777 - val_loss: 0.6196\nEpoch 12/30\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7789 - loss: 0.6489\nEpoch 12: val_loss did not improve from 0.60035\n\u001b[1m2161/2161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 104ms/step - accuracy: 0.7789 - loss: 0.6489 - val_accuracy: 0.7777 - val_loss: 0.6343\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x78b4e4764090>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}